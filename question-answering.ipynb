{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-07T05:06:41.109560Z","iopub.execute_input":"2022-11-07T05:06:41.110076Z","iopub.status.idle":"2022-11-07T05:06:41.141326Z","shell.execute_reply.started":"2022-11-07T05:06:41.109973Z","shell.execute_reply":"2022-11-07T05:06:41.139882Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/chaii-hindi-and-tamil-question-answering/sample_submission.csv\n/kaggle/input/chaii-hindi-and-tamil-question-answering/train.csv\n/kaggle/input/chaii-hindi-and-tamil-question-answering/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers","metadata":{"execution":{"iopub.status.busy":"2022-11-06T22:35:54.793117Z","iopub.execute_input":"2022-11-06T22:35:54.793852Z","iopub.status.idle":"2022-11-06T22:36:07.724872Z","shell.execute_reply.started":"2022-11-06T22:35:54.793815Z","shell.execute_reply":"2022-11-06T22:36:07.723453Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.4.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:06:46.257475Z","iopub.execute_input":"2022-11-07T05:06:46.257923Z","iopub.status.idle":"2022-11-07T05:06:47.023937Z","shell.execute_reply.started":"2022-11-07T05:06:46.257888Z","shell.execute_reply":"2022-11-07T05:06:47.022467Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:06:50.233543Z","iopub.execute_input":"2022-11-07T05:06:50.233998Z","iopub.status.idle":"2022-11-07T05:06:57.572161Z","shell.execute_reply.started":"2022-11-07T05:06:50.233963Z","shell.execute_reply":"2022-11-07T05:06:57.570822Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575d9629bcf147fc9694f6915b30c9b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/5.38M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36cb46b44df9495e80117a1dd159f935"}},"metadata":{}}]},{"cell_type":"code","source":"pad_on_right = tokenizer.padding_side == \"right\"","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:06:59.395250Z","iopub.execute_input":"2022-11-07T05:06:59.395700Z","iopub.status.idle":"2022-11-07T05:06:59.402436Z","shell.execute_reply.started":"2022-11-07T05:06:59.395665Z","shell.execute_reply":"2022-11-07T05:06:59.400841Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('/kaggle/input/chaii-hindi-and-tamil-question-answering/train.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:07:05.771212Z","iopub.execute_input":"2022-11-07T05:07:05.772718Z","iopub.status.idle":"2022-11-07T05:07:06.474401Z","shell.execute_reply.started":"2022-11-07T05:07:05.772660Z","shell.execute_reply":"2022-11-07T05:07:06.472389Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"          id                                            context  \\\n0  903deec17  ஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...   \n1  d9841668c  காளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...   \n2  29d154b56  சர் அலெக்ஸாண்டர் ஃபிளெமிங் (Sir Alexander Flem...   \n3  41660850a  குழந்தையின் அழுகையை  நிறுத்தவும், தூங்க வைக்கவ...   \n4  b29c82c22  சூரியக் குடும்பம் \\nசூரியக் குடும்பம் (Solar S...   \n\n                                            question  \\\n0               மனித உடலில் எத்தனை எலும்புகள் உள்ளன?   \n1                         காளிதாசன் எங்கு பிறந்தார்?   \n2                   பென்சிலின் கண்டுபிடித்தவர் யார்?   \n3  தமிழ்நாட்டில் குழந்தைகளை தூங்க வைக்க பாடும் பா...   \n4                பூமியின் அருகில் உள்ள விண்மீன் எது?   \n\n                  answer_text  answer_start language  \n0                         206            53    tamil  \n1                  காசுமீரில்          2358    tamil  \n2  சர் அலெக்ஸாண்டர் ஃபிளெமிங்             0    tamil  \n3                    தாலாட்டு            68    tamil  \n4                   சூரியனும்           585    tamil  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answer_text</th>\n      <th>answer_start</th>\n      <th>language</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>903deec17</td>\n      <td>ஒரு சாதாரண வளர்ந்த மனிதனுடைய எலும்புக்கூடு பின...</td>\n      <td>மனித உடலில் எத்தனை எலும்புகள் உள்ளன?</td>\n      <td>206</td>\n      <td>53</td>\n      <td>tamil</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d9841668c</td>\n      <td>காளிதாசன் (தேவநாகரி: कालिदास) சமஸ்கிருத இலக்கி...</td>\n      <td>காளிதாசன் எங்கு பிறந்தார்?</td>\n      <td>காசுமீரில்</td>\n      <td>2358</td>\n      <td>tamil</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>29d154b56</td>\n      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங் (Sir Alexander Flem...</td>\n      <td>பென்சிலின் கண்டுபிடித்தவர் யார்?</td>\n      <td>சர் அலெக்ஸாண்டர் ஃபிளெமிங்</td>\n      <td>0</td>\n      <td>tamil</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>41660850a</td>\n      <td>குழந்தையின் அழுகையை  நிறுத்தவும், தூங்க வைக்கவ...</td>\n      <td>தமிழ்நாட்டில் குழந்தைகளை தூங்க வைக்க பாடும் பா...</td>\n      <td>தாலாட்டு</td>\n      <td>68</td>\n      <td>tamil</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>b29c82c22</td>\n      <td>சூரியக் குடும்பம் \\nசூரியக் குடும்பம் (Solar S...</td>\n      <td>பூமியின் அருகில் உள்ள விண்மீன் எது?</td>\n      <td>சூரியனும்</td>\n      <td>585</td>\n      <td>tamil</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def convert_answers(row):\n    return {\"answer_start\": [row[0]], \"text\": [row[1]]}","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:07:14.521079Z","iopub.execute_input":"2022-11-07T05:07:14.521591Z","iopub.status.idle":"2022-11-07T05:07:14.529232Z","shell.execute_reply.started":"2022-11-07T05:07:14.521552Z","shell.execute_reply":"2022-11-07T05:07:14.527217Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def prepare_train_features(examples, max_length=384, doc_stride=192):\n    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n    # left whitespace\n    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n\n    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\" if pad_on_right else \"context\"],\n        examples[\"context\" if pad_on_right else \"question\"],\n        truncation=\"only_second\" if pad_on_right else \"only_first\",\n        max_length=max_length,\n        stride=doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    # Since one example might give us several features if it has a long context, we need a map from a feature to\n    # its corresponding example. This key gives us just that.\n    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        sample_index = sample_mapping[i]\n        answers = examples[\"answers\"][sample_index]\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n                token_start_index += 1\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n                token_end_index -= 1\n\n            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                tokenized_examples[\"start_positions\"].append(cls_index)\n                tokenized_examples[\"end_positions\"].append(cls_index)\n            else:\n                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n                # Note: we could go after the last offset if the answer is the last word (edge case).\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n\n    return tokenized_examples","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:07:20.273466Z","iopub.execute_input":"2022-11-07T05:07:20.273887Z","iopub.status.idle":"2022-11-07T05:07:20.291376Z","shell.execute_reply.started":"2022-11-07T05:07:20.273855Z","shell.execute_reply":"2022-11-07T05:07:20.290135Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df[\"answers\"] = df[[\"answer_start\", \"answer_text\"]].apply(convert_answers, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:07:25.933173Z","iopub.execute_input":"2022-11-07T05:07:25.934732Z","iopub.status.idle":"2022-11-07T05:07:25.963580Z","shell.execute_reply.started":"2022-11-07T05:07:25.934664Z","shell.execute_reply":"2022-11-07T05:07:25.962532Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset\n\ndataset = Dataset.from_pandas(df)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:07:30.793842Z","iopub.execute_input":"2022-11-07T05:07:30.794328Z","iopub.status.idle":"2022-11-07T05:07:31.731702Z","shell.execute_reply.started":"2022-11-07T05:07:30.794292Z","shell.execute_reply":"2022-11-07T05:07:31.730687Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = dataset.map(\n    prepare_train_features,\n    batched=True,\n    remove_columns=dataset.column_names,\n    num_proc=3,\n)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:07:35.607735Z","iopub.execute_input":"2022-11-07T05:07:35.608518Z","iopub.status.idle":"2022-11-07T05:07:56.935548Z","shell.execute_reply.started":"2022-11-07T05:07:35.608459Z","shell.execute_reply":"2022-11-07T05:07:56.933897Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"    ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#0:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a87b1677c2c450cb2f6590de7dc572b"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#1:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a06e109abd14d3f8ef457b0937154f6"}},"metadata":{}},{"name":"stdout","text":" ","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"#2:   0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef8d5bcc20ea434db8f8fbb20626d92a"}},"metadata":{}}]},{"cell_type":"code","source":"train_set = tokenized_datasets.with_format(\"numpy\")[:]","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:08:04.002219Z","iopub.execute_input":"2022-11-07T05:08:04.002733Z","iopub.status.idle":"2022-11-07T05:08:04.446555Z","shell.execute_reply.started":"2022-11-07T05:08:04.002693Z","shell.execute_reply":"2022-11-07T05:08:04.445202Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ntrain_set['input_ids']=np.array([i.astype(np.int32) for i in train_set['input_ids']])\ntrain_set['token_type_ids']=np.array([i.astype(np.int32) for i in train_set['token_type_ids']])\ntrain_set['attention_mask']=np.array([i.astype(np.int32) for i in train_set['attention_mask']])","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:08:22.711857Z","iopub.execute_input":"2022-11-07T05:08:22.712897Z","iopub.status.idle":"2022-11-07T05:08:22.941982Z","shell.execute_reply.started":"2022-11-07T05:08:22.712849Z","shell.execute_reply":"2022-11-07T05:08:22.940790Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import TFAutoModelForQuestionAnswering\n\nmodel = TFAutoModelForQuestionAnswering.from_pretrained(\"ai4bharat/indic-bert\", from_pt=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-08T14:13:52.146071Z","iopub.execute_input":"2022-11-08T14:13:52.146470Z","iopub.status.idle":"2022-11-08T14:14:06.114304Z","shell.execute_reply.started":"2022-11-08T14:13:52.146439Z","shell.execute_reply":"2022-11-08T14:14:06.113255Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/507 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af9501658ad24970b28b3d70cedee464"}},"metadata":{}},{"name":"stderr","text":"2022-11-08 14:14:01.493242: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/129M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be6a8a929e3d40679c3d3c4b83967fe5"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFAlbertForQuestionAnswering: ['sop_classifier.classifier.weight', 'sop_classifier.classifier.bias']\n- This IS expected if you are initializing TFAlbertForQuestionAnswering from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFAlbertForQuestionAnswering from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFAlbertForQuestionAnswering were not initialized from the PyTorch model and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\noptimizer = keras.optimizers.Adam(learning_rate=5e-5)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:09:27.783033Z","iopub.execute_input":"2022-11-07T05:09:27.783524Z","iopub.status.idle":"2022-11-07T05:09:27.793823Z","shell.execute_reply.started":"2022-11-07T05:09:27.783482Z","shell.execute_reply":"2022-11-07T05:09:27.792338Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Optionally uncomment the next line for float16 training\n#keras.mixed_precision.set_global_policy(\"mixed_float32\")\n\nmodel.compile(optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:09:30.129242Z","iopub.execute_input":"2022-11-07T05:09:30.130794Z","iopub.status.idle":"2022-11-07T05:09:30.149057Z","shell.execute_reply.started":"2022-11-07T05:09:30.130739Z","shell.execute_reply":"2022-11-07T05:09:30.147447Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:09:33.278526Z","iopub.execute_input":"2022-11-07T05:09:33.278984Z","iopub.status.idle":"2022-11-07T05:09:33.287465Z","shell.execute_reply.started":"2022-11-07T05:09:33.278947Z","shell.execute_reply":"2022-11-07T05:09:33.286047Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"tf_albert_for_question_answering\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nalbert (TFAlbertMainLayer)   multiple                  32852992  \n_________________________________________________________________\nqa_outputs (Dense)           multiple                  1538      \n=================================================================\nTotal params: 32,854,530\nTrainable params: 32,854,530\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(train_set, epochs=1, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:23:37.062882Z","iopub.execute_input":"2022-11-07T05:23:37.063379Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":" 75/747 [==>...........................] - ETA: 11:47:35 - loss: 2.4265","output_type":"stream"}]},{"cell_type":"code","source":"context = \"\"\"Once upon a time in Japan there was a girl. Her name was Jujitsu \"\"\"\nquestion = \"What was the name of the girl in Japan?\"\n\ninputs = tokenizer([context], [question], return_tensors=\"np\")\noutputs = model(inputs)\nstart_position = tf.argmax(outputs.start_logits, axis=1)\nend_position = tf.argmax(outputs.end_logits, axis=1)\nprint(int(start_position), int(end_position[0]))","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:23:17.642366Z","iopub.execute_input":"2022-11-07T05:23:17.645417Z","iopub.status.idle":"2022-11-07T05:23:17.972378Z","shell.execute_reply.started":"2022-11-07T05:23:17.645315Z","shell.execute_reply":"2022-11-07T05:23:17.971442Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"0 20\n","output_type":"stream"}]},{"cell_type":"code","source":"answer = inputs[\"input_ids\"][0, int(start_position) : int(end_position) + 1]\nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:23:19.463491Z","iopub.execute_input":"2022-11-07T05:23:19.463964Z","iopub.status.idle":"2022-11-07T05:23:19.472380Z","shell.execute_reply.started":"2022-11-07T05:23:19.463927Z","shell.execute_reply":"2022-11-07T05:23:19.470628Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[    2  6136 11603    33   626    26 27889 43982   671    95    33  6846\n     9   481  5109    95 50886 14737 40501     8     3]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(tokenizer.decode(answer))","metadata":{"execution":{"iopub.status.busy":"2022-11-07T05:23:24.679099Z","iopub.execute_input":"2022-11-07T05:23:24.679617Z","iopub.status.idle":"2022-11-07T05:23:24.688266Z","shell.execute_reply.started":"2022-11-07T05:23:24.679579Z","shell.execute_reply":"2022-11-07T05:23:24.686749Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"[CLS] once upon a time in japan there was a girl. her name was jujitsu [SEP]\n","output_type":"stream"}]}]}